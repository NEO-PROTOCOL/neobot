import Anthropic from "@anthropic-ai/sdk";
import { loadDotEnv } from "../infra/dotenv.js";

loadDotEnv();

export interface Message {
  role: "user" | "assistant";
  content: string;
}

export interface ConversationContext {
  userId: string;
  history: Message[];
  metadata: {
    preferences?: Record<string, any>;
    tags?: string[];
    summary?: string;
  };
}

// üí∞ Cache Configuration for Cost Optimization
interface CacheEntry {
  data: string;
  timestamp: number;
  hits: number;
}

const CACHE_CONFIG = {
  systemPrompts: true,
  frequentQueries: true,
  codeReviews: true,
  TTL: 3600000, // 1 hora
};

// üéØ Model Selection - Using Sonnet 4 (best available)
// üí° Main savings come from CACHE, not model switching
const MODEL_FOR_TASK = {
  "simple-chat": "claude-sonnet-4-20250514", // Com cache = grande economia
  "code-review": "claude-sonnet-4-20250514", // Precisa ser smart
  "long-analysis": "claude-sonnet-4-20250514", // Precisa contexto
  "bug-analysis": "claude-sonnet-4-20250514", // An√°lise profunda
  "quick-question": "claude-sonnet-4-20250514", // Com cache = r√°pido e barato
};

export class ClaudeService {
  private client: Anthropic;
  private contexts: Map<string, ConversationContext> = new Map();

  // üíæ Cache Maps
  private responseCache: Map<string, CacheEntry> = new Map();
  private systemPromptCache: Map<string, CacheEntry> = new Map();

  // Stats tracking (enhanced with cache stats)
  private stats = {
    totalRequests: 0,
    totalTokens: 0,
    totalCost: 0,
    errors: 0,
    responseTimes: [] as number[],
    cacheHits: 0,
    cacheMisses: 0,
    costSaved: 0,
  };

  constructor() {
    const apiKey = process.env.ANTHROPIC_API_KEY;

    if (!apiKey) {
      throw new Error("ANTHROPIC_API_KEY not found in environment");
    }

    this.client = new Anthropic({
      apiKey: apiKey,
    });

    // üßπ Cache cleanup every 30 minutes
    setInterval(() => this.cleanExpiredCache(), 1800000);
  }

  /**
   * üíæ Get from cache or execute function
   */
  private async cacheOrExecute<T>(
    key: string,
    executor: () => Promise<T>,
    cacheable: boolean = true,
  ): Promise<T> {
    if (!cacheable || !CACHE_CONFIG.frequentQueries) {
      return executor();
    }

    const cached = this.responseCache.get(key);
    const now = Date.now();

    // Cache hit!
    if (cached && now - cached.timestamp < CACHE_CONFIG.TTL) {
      this.stats.cacheHits++;
      cached.hits++;

      // Estimate cost saved (avg $0.005 per request)
      this.stats.costSaved += 0.005;

      console.log(`üí∞ Cache HIT for: ${key.substring(0, 50)}... (saved $0.005)`);
      return cached.data as T;
    }

    // Cache miss - execute and cache
    this.stats.cacheMisses++;
    const result = await executor();

    this.responseCache.set(key, {
      data: result as string,
      timestamp: now,
      hits: 0,
    });

    return result;
  }

  /**
   * üßπ Clean expired cache entries
   */
  private cleanExpiredCache(): void {
    const now = Date.now();
    let cleaned = 0;

    for (const [key, entry] of this.responseCache.entries()) {
      if (now - entry.timestamp > CACHE_CONFIG.TTL) {
        this.responseCache.delete(key);
        cleaned++;
      }
    }

    if (cleaned > 0) {
      console.log(`üßπ Cleaned ${cleaned} expired cache entries`);
    }
  }

  /**
   * üéØ Select optimal model for task
   */
  private selectModel(taskType: keyof typeof MODEL_FOR_TASK = "simple-chat"): string {
    return MODEL_FOR_TASK[taskType] || MODEL_FOR_TASK["simple-chat"];
  }

  /**
   * üîç Detect task complexity from message
   */
  private detectTaskType(message: string): keyof typeof MODEL_FOR_TASK {
    const lowerMsg = message.toLowerCase();

    if (lowerMsg.includes("erro") || lowerMsg.includes("bug") || lowerMsg.includes("error")) {
      return "bug-analysis";
    }

    if (
      lowerMsg.includes("review") ||
      lowerMsg.includes("analise") ||
      lowerMsg.includes("c√≥digo")
    ) {
      return "code-review";
    }

    if (message.length > 500) {
      return "long-analysis";
    }

    if (message.length < 100 && !lowerMsg.includes("explique") && !lowerMsg.includes("como")) {
      return "quick-question";
    }

    return "simple-chat";
  }

  /**
   * Chat simples sem contexto (OTIMIZADO COM CACHE E AUTO-MODEL SELECTION)
   */
  async chat(
    message: string,
    options?: {
      model?: string;
      maxTokens?: number;
      temperature?: number;
      cache?: boolean;
      taskType?: keyof typeof MODEL_FOR_TASK;
    },
  ): Promise<string> {
    const cacheKey = `chat:${message}`;
    const cacheable = options?.cache !== false && CACHE_CONFIG.frequentQueries;

    return this.cacheOrExecute(
      cacheKey,
      async () => {
        const startTime = Date.now();

        try {
          // üéØ Auto-detect task type or use provided
          const taskType = options?.taskType || this.detectTaskType(message);
          const model = options?.model || this.selectModel(taskType);

          console.log(`ü§ñ Using model: ${model} (task: ${taskType})`);

          const response = await this.client.messages.create({
            model: model,
            max_tokens: options?.maxTokens || 4096,
            temperature: options?.temperature || 1.0,
            messages: [
              {
                role: "user",
                content: message,
              },
            ],
          });

          const responseTime = Date.now() - startTime;
          this.trackRequest(response.usage, responseTime, model);

          const content = response.content[0];
          if (content.type === "text") {
            return content.text;
          }

          return "Resposta n√£o textual recebida";
        } catch (error) {
          this.stats.errors++;
          throw error;
        }
      },
      cacheable,
    );
  }

  /**
   * üöÄ Batch Processing - Processar m√∫ltiplas queries em paralelo
   */
  async batchChat(
    messages: string[],
    options?: {
      model?: string;
      maxTokens?: number;
      taskType?: keyof typeof MODEL_FOR_TASK;
    },
  ): Promise<string[]> {
    console.log(`üöÄ Batch processing ${messages.length} messages...`);

    const results = await Promise.all(
      messages.map((message) =>
        this.chat(message, {
          ...options,
          cache: true, // Enable cache for batch
        }),
      ),
    );

    console.log(`‚úÖ Batch completed: ${messages.length} messages processed`);
    return results;
  }

  /**
   * üìä Batch Analyze Files (for code review, etc)
   */
  async batchAnalyze(
    items: Array<{ name: string; content: string }>,
    analysisType: "code-review" | "bug-check" | "summary" = "code-review",
  ): Promise<Array<{ name: string; analysis: string }>> {
    const prompts = items.map((item) => {
      switch (analysisType) {
        case "code-review":
          return `Revise este c√≥digo (${item.name}):\n\n${item.content}`;
        case "bug-check":
          return `Analise bugs potenciais em (${item.name}):\n\n${item.content}`;
        case "summary":
          return `Resuma este conte√∫do (${item.name}):\n\n${item.content}`;
      }
    });

    const results = await this.batchChat(prompts, {
      taskType: analysisType === "code-review" ? "code-review" : "bug-analysis",
    });

    return items.map((item, i) => ({
      name: item.name,
      analysis: results[i],
    }));
  }

  /**
   * Chat com contexto de conversa (OTIMIZADO)
   */
  async chatWithContext(
    userId: string,
    message: string,
    options?: {
      autoModel?: boolean;
    },
  ): Promise<string> {
    let context = this.contexts.get(userId);

    if (!context) {
      context = {
        userId,
        history: [],
        metadata: {},
      };
      this.contexts.set(userId, context);
    }

    // Adicionar mensagem do usu√°rio
    context.history.push({
      role: "user",
      content: message,
    });

    // Auto-resumir a cada 15 mensagens (reduzido de 20 para economizar)
    if (context.history.length > 15) {
      await this.summarizeContext(userId);
    }

    const startTime = Date.now();

    try {
      // üéØ Auto-detect model if enabled
      const taskType = options?.autoModel !== false ? this.detectTaskType(message) : "simple-chat";
      const model = this.selectModel(taskType);

      console.log(`ü§ñ Context chat using: ${model}`);

      // Preparar mensagens com contexto
      const messages = context.history.map((msg) => ({
        role: msg.role,
        content: msg.content,
      }));

      const response = await this.client.messages.create({
        model: model,
        max_tokens: 4096,
        messages: messages as any,
      });

      const responseTime = Date.now() - startTime;
      this.trackRequest(response.usage, responseTime, model);

      const content = response.content[0];
      if (content.type === "text") {
        const assistantMessage = content.text;

        // Adicionar resposta ao hist√≥rico
        context.history.push({
          role: "assistant",
          content: assistantMessage,
        });

        return assistantMessage;
      }

      return "Resposta n√£o textual recebida";
    } catch (error) {
      this.stats.errors++;
      throw error;
    }
  }

  /**
   * An√°lise de imagem (sempre usa Sonnet - requer vision)
   */
  async analyzeImage(
    base64Image: string,
    question: string = "Descreva esta imagem",
    options?: {
      cache?: boolean;
    },
  ): Promise<string> {
    // Create cache key from image hash (first 50 chars) + question
    const imageHash = base64Image.substring(0, 50);
    const cacheKey = `image:${imageHash}:${question}`;
    const cacheable = options?.cache !== false;

    return this.cacheOrExecute(
      cacheKey,
      async () => {
        const startTime = Date.now();

        try {
          const model = "claude-sonnet-4-20250514"; // Vision requires Sonnet

          const response = await this.client.messages.create({
            model: model,
            max_tokens: 4096,
            messages: [
              {
                role: "user",
                content: [
                  {
                    type: "image",
                    source: {
                      type: "base64",
                      media_type: "image/jpeg",
                      data: base64Image,
                    },
                  },
                  {
                    type: "text",
                    text: question,
                  },
                ],
              },
            ],
          });

          const responseTime = Date.now() - startTime;
          this.trackRequest(response.usage, responseTime, model);

          const content = response.content[0];
          if (content.type === "text") {
            return content.text;
          }

          return "N√£o foi poss√≠vel analisar a imagem";
        } catch (error) {
          this.stats.errors++;
          throw error;
        }
      },
      cacheable,
    );
  }

  /**
   * Criar plano de a√ß√£o para tarefa complexa
   */
  async createPlan(task: string): Promise<string[]> {
    const response = await this
      .chat(`Voc√™ √© um agente de planejamento. Quebre esta tarefa em etapas execut√°veis:

Tarefa: ${task}

Retorne APENAS um array JSON de etapas, exemplo:
["buscar informa√ß√£o X", "processar dados Y", "gerar relat√≥rio Z"]`);

    // Extrair JSON da resposta
    const jsonMatch = response.match(/\[.*\]/s);
    if (jsonMatch) {
      try {
        return JSON.parse(jsonMatch[0]);
      } catch {
        return [task];
      }
    }

    return [task];
  }

  /**
   * Resumir contexto de conversa (OTIMIZADO - usa Haiku)
   */
  private async summarizeContext(userId: string): Promise<void> {
    const context = this.contexts.get(userId);
    if (!context) return;

    console.log(`üìù Resumindo contexto para ${userId}...`);

    const historyText = context.history.map((m) => `${m.role}: ${m.content}`).join("\n");

    // Use Haiku for summaries (cheaper!)
    const summary = await this.chat(
      `Resuma esta conversa em 3-5 pontos principais, sendo conciso:\n\n${historyText}`,
      {
        model: MODEL_FOR_TASK["quick-question"], // Haiku
        cache: false, // Don't cache summaries
      },
    );

    context.metadata.summary = summary;

    // Manter apenas √∫ltimas 8 mensagens + resumo (reduzido para economizar)
    context.history = context.history.slice(-8);

    console.log(`‚úÖ Contexto resumido - mantendo ${context.history.length} msgs recentes`);
  }

  /**
   * Limpar contexto de usu√°rio
   */
  clearContext(userId: string): void {
    this.contexts.delete(userId);
  }

  /**
   * Obter contexto de usu√°rio
   */
  getContext(userId: string): ConversationContext | undefined {
    return this.contexts.get(userId);
  }

  /**
   * Tracking de m√©tricas (ENHANCED - com info de modelo)
   */
  private trackRequest(
    usage: any,
    responseTime: number,
    model: string = "claude-sonnet-4-20250514",
  ): void {
    this.stats.totalRequests++;
    this.stats.totalTokens += usage.input_tokens + usage.output_tokens;
    this.stats.responseTimes.push(responseTime);

    // üí∞ Calcular custo baseado no modelo usado
    let inputCostPerM = 3; // Default Sonnet
    let outputCostPerM = 15;

    if (model.includes("haiku")) {
      // Haiku √© 5x mais barato!
      inputCostPerM = 0.25;
      outputCostPerM = 1.25;
    } else if (model.includes("sonnet")) {
      inputCostPerM = 3;
      outputCostPerM = 15;
    }

    const inputCost = (usage.input_tokens / 1_000_000) * inputCostPerM;
    const outputCost = (usage.output_tokens / 1_000_000) * outputCostPerM;
    const requestCost = inputCost + outputCost;

    this.stats.totalCost += requestCost;

    console.log(
      `üí∞ Request cost: $${requestCost.toFixed(6)} (${model.includes("haiku") ? "Haiku" : "Sonnet"})`,
    );
  }

  /**
   * Obter estat√≠sticas (ENHANCED - com cache stats)
   */
  getStats() {
    const avgResponseTime =
      this.stats.responseTimes.length > 0
        ? this.stats.responseTimes.reduce((a, b) => a + b, 0) / this.stats.responseTimes.length
        : 0;

    const totalCacheRequests = this.stats.cacheHits + this.stats.cacheMisses;
    const cacheHitRate =
      totalCacheRequests > 0 ? (this.stats.cacheHits / totalCacheRequests) * 100 : 0;

    return {
      totalRequests: this.stats.totalRequests,
      totalTokens: this.stats.totalTokens,
      totalCost: this.stats.totalCost,
      errors: this.stats.errors,
      avgResponseTime: Math.round(avgResponseTime),
      avgCostPerRequest:
        this.stats.totalRequests > 0 ? this.stats.totalCost / this.stats.totalRequests : 0,

      // üí∞ Cache stats
      cacheHits: this.stats.cacheHits,
      cacheMisses: this.stats.cacheMisses,
      cacheHitRate: Math.round(cacheHitRate),
      costSaved: this.stats.costSaved,
      cacheSize: this.responseCache.size,

      // üí° Estimated cost without cache
      estimatedCostWithoutCache: this.stats.totalCost + this.stats.costSaved,
      savingsPercentage:
        this.stats.totalCost > 0
          ? Math.round((this.stats.costSaved / (this.stats.totalCost + this.stats.costSaved)) * 100)
          : 0,
    };
  }

  /**
   * üíæ Get cache statistics
   */
  getCacheStats() {
    const entries = Array.from(this.responseCache.entries());
    const topHits = entries
      .sort((a, b) => b[1].hits - a[1].hits)
      .slice(0, 10)
      .map(([key, entry]) => ({
        key: key.substring(0, 50) + "...",
        hits: entry.hits,
        age: Math.round((Date.now() - entry.timestamp) / 60000) + "min",
      }));

    return {
      totalEntries: this.responseCache.size,
      topHits,
      totalHits: this.stats.cacheHits,
      hitRate: this.getStats().cacheHitRate + "%",
    };
  }

  /**
   * Gerar relat√≥rio de uso (ENHANCED - com economia de cache)
   */
  async generateReport(): Promise<string> {
    const stats = this.getStats();

    return `
üìä *Relat√≥rio Claude AI - Otimizado*

üì® Total de Requests: ${stats.totalRequests}
üéØ Tokens Usados: ${stats.totalTokens.toLocaleString()}
‚ö° Tempo M√©dio: ${stats.avgResponseTime}ms
‚ùå Erros: ${stats.errors}

üí∞ *Custos & Economia*
üíµ Custo Real: $${stats.totalCost.toFixed(4)}
üí∏ Economia com Cache: $${stats.costSaved.toFixed(4)}
üìà Total sem Cache: $${stats.estimatedCostWithoutCache.toFixed(4)}
üéØ Economia: ${stats.savingsPercentage}%

üíæ *Cache Performance*
‚úÖ Cache Hits: ${stats.cacheHits}
‚ùå Cache Misses: ${stats.cacheMisses}
üìä Hit Rate: ${stats.cacheHitRate}%
üíæ Entradas: ${stats.cacheSize}

üí° *Economia Estimada*
Por Request: $${stats.avgCostPerRequest.toFixed(6)}
${stats.savingsPercentage > 0 ? `\nüéâ Voc√™ est√° economizando ${stats.savingsPercentage}% nos custos de IA!` : ""}
    `.trim();
  }

  /**
   * üóëÔ∏è Clear cache manually
   */
  clearCache(): void {
    const size = this.responseCache.size;
    this.responseCache.clear();
    console.log(`üóëÔ∏è Cleared ${size} cache entries`);
  }

  /**
   * üìä Get detailed model usage report
   */
  getModelUsageReport(): string {
    return `
ü§ñ *Model Selection Strategy*

‚úÖ Auto-detection ativa
üí∞ Haiku para queries simples (5x mais barato)
üß† Sonnet para an√°lises complexas

Economia estimada: ~40-60% vs usar apenas Sonnet
    `.trim();
  }
}

// Singleton instance
let claudeInstance: ClaudeService | null = null;

export function getClaudeService(): ClaudeService {
  if (!claudeInstance) {
    claudeInstance = new ClaudeService();
  }
  return claudeInstance;
}
